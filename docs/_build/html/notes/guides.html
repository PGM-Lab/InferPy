

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Guides &mdash; InferPy 1.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../_static/css/inferpy_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="InferPy 1.0 documentation" href="../index.html"/>
        <link rel="next" title="Probabilistic Model Zoo" href="probzoo.html"/>
        <link rel="prev" title="Getting Started" href="gettingstarted.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo-doc.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Docs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gettingstarted.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gettingstarted.html#seconds-to-inferpy">30 seconds to InferPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="gettingstarted.html#guiding-principles">Guiding Principles</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Guides</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#guide-to-building-probabilistic-models">Guide to Building Probabilistic Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#guide-to-approximate-inference-in-probabilistic-models">Guide to Approximate Inference in Probabilistic Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#guide-to-bayesian-deep-learning">Guide to Bayesian Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#guide-to-validation-of-probabilistic-models">Guide to Validation of Probabilistic Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#guide-to-data-handling">Guide to Data Handling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="probzoo.html">Probabilistic Model Zoo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#bayesian-linear-regression">Bayesian Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#zero-inflated-linear-regression">Zero Inflated Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#bayesian-logistic-regression">Bayesian Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#bayesian-multinomial-logistic-regression">Bayesian Multinomial Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#mixture-of-gaussians">Mixture of Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#linear-factor-model-pca">Linear Factor Model (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#pca-with-ard-prior-pca">PCA with ARD Prior (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#mixed-membership-model">Mixed Membership Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#latent-dirichlet-allocation">Latent Dirichlet Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#matrix-factorization">Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#linear-mixed-effect-model">Linear Mixed Effect Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#bayesian-neural-network-classifier">Bayesian Neural Network Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="probzoo.html#variational-autoencoder">Variational Autoencoder</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">InferPy</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Guides</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notes/guides.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="guides">
<h1>Guides<a class="headerlink" href="#guides" title="Permalink to this headline">¶</a></h1>
<div class="section" id="guide-to-building-probabilistic-models">
<h2>Guide to Building Probabilistic Models<a class="headerlink" href="#guide-to-building-probabilistic-models" title="Permalink to this headline">¶</a></h2>
<p>InferPy focuses on <em>hirearchical probabilistic models</em> which usually are
structured in two different layers:</p>
<ul class="simple">
<li>A <strong>prior model</strong> defining a joint distribution <span class="math">\(p(\theta)\)</span>
over the global parameters of the model, <span class="math">\(\theta\)</span>.</li>
<li>A <strong>data or observation model</strong> defining a joint conditional
distribution <span class="math">\(p(x,h|\theta)\)</span> over the observed quantities
<span class="math">\(x\)</span> and the the local hidden variables <span class="math">\(h\)</span> governing the
observation <span class="math">\(x\)</span>. This data model should be specified in a
single-sample basis. There are many models of interest without local
hidden variables, in that case we simply specify the conditional
<span class="math">\(p(x|\theta)\)</span>. More flexible ways of defining the data model
can be found in ?.</li>
</ul>
<p>A Bayesian PCA model has the following graphical structure,</p>
<div class="figure align-center" id="id5">
<a class="reference internal image-reference" href="../_images/LinearFactor.png"><img alt="Linear Factor Model" src="../_images/LinearFactor.png" style="width: 161.0px; height: 225.0px;" /></a>
<p class="caption"><span class="caption-text">Bayesian PCA</span></p>
<div class="legend">
<blockquote>
<div>The <strong>prior model</strong> is the variable <span class="math">\(\mu\)</span>. The <strong>data model</strong> is the part of the model surrounded by the box indexed by <strong>N</strong>.</div></blockquote>
</div>
</div>
<p>And this is how this Bayesian PCA model is denfined in InferPy:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">inferpy</span> <span class="k">as</span> <span class="nn">inf</span>
<span class="kn">from</span> <span class="nn">inferpy.models</span> <span class="k">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">InverseGamma</span><span class="p">,</span> <span class="n">Dirichlet</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">inferpy</span> <span class="k">as</span> <span class="nn">inf</span>
<span class="kn">from</span> <span class="nn">inferpy.models</span> <span class="k">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">InverseGamma</span><span class="p">,</span> <span class="n">Dirichlet</span>

    <span class="k">with</span> <span class="n">inf</span><span class="o">.</span><span class="n">ProbModel</span><span class="p">()</span> <span class="k">as</span> <span class="n">pca</span><span class="p">:</span>
   <span class="c1"># K defines the number of components.</span>
   <span class="n">K</span><span class="o">=</span><span class="mi">10</span>

   <span class="c1">#Prior for the principal components</span>
   <span class="k">with</span> <span class="n">inf</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="n">K</span><span class="p">)</span>
      <span class="n">mu</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dime</span> <span class="o">=</span> <span class="n">d</span><span class="p">)</span>

   <span class="c1"># Prior for the intercept component</span>
   <span class="n">mu0</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">d</span><span class="p">)</span>

   <span class="c1"># Variance of the Normal</span>
   <span class="n">variance</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="c1"># Number of observations</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1">#data Model</span>
<span class="k">with</span> <span class="n">inf</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="n">N</span><span class="p">):</span>
    <span class="c1"># Latent representation of the sample</span>
    <span class="n">w_n</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">K</span><span class="p">)</span>
    <span class="c1"># Observed sample. The dimensionality of mu is [K,d].</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">mu0</span> <span class="o">+</span> <span class="n">inf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w_n</span><span class="p">,</span><span class="n">mu</span><span class="p">),</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">variance</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>

<span class="c1">#compile the probabilistic model</span>
<span class="n">pca</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">with</span> <span class="pre">inf.replicate(size</span> <span class="pre">=</span> <span class="pre">N)</span></code> sintaxis is used to replicate the
random variables contained within this construct. It follows from the
so-called <em>plateau notation</em> to define the data generation part of a
probabilistic model. Every replicated variable is <strong>conditionally
idependent</strong> given the previous random variables (if any) defined
outside the <strong>with</strong> statement.</p>
<p>Following Edward&#8217;s approach, a random variable <span class="math">\(x\)</span> is an object
parametrized by a tensor <span class="math">\(\theta\)</span> (i.e. a TensorFlow&#8217;s tensor or
numpy&#8217;s ndarray). The number of random variables in one object is
determined by the dimensions of its parameters (like in Edward) or by
the &#8216;shape&#8217; or &#8216;dim&#8217; argument (inspired by PyMC3 and Keras):</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># matrix of [1, 5] univariate standard normals</span>
<span class="n">x</span>  <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># matrix of [1, 5] univariate standard normals</span>
<span class="n">x</span>  <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

<span class="c1"># matrix of [1,5] univariate standard normals</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Normal</span> <span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">with</span> <span class="pre">inf.replicate(size</span> <span class="pre">=</span> <span class="pre">N)</span></code> sintaxis can also be used to define
multi-dimensional objects:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># matrix of [10,5] univariate standard normals</span>
<span class="k">with</span> <span class="n">inf</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Normal</span> <span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Multivariate distributions can be defined similarly. Following Edward&#8217;s
approach, the multivariate dimension is the innermost (right-most)
dimension of the parameters.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Object with five K-dimensional multivariate normals, shape(x) = [5,K]</span>
<span class="n">x</span>  <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="n">K</span><span class="p">]),</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">]))</span>

<span class="c1"># Object with five K-dimensional multivariate normals, shape(x) = [5,K]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MultivariateNormal</span> <span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">),</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">]),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="n">K</span><span class="p">])</span>
</pre></div>
</div>
<p>The argument <code class="docutils literal"><span class="pre">observed</span> <span class="pre">=</span> <span class="pre">true</span></code> in the constructor of a random variable
is used to indicate whether a variable is observable or not.</p>
<p>A <strong>probabilistic model</strong> defines a joint distribution over observable
and non-observable variables, <span class="math">\(p(\theta,\mu,\sigma,z_n, x_n)\)</span> for the
running example. The variables in the model are the ones defined using the
<a href="#id1"><span class="problematic" id="id2">``</span></a>with inf.ProbModel() as pca:<a href="#id3"><span class="problematic" id="id4">``</span></a>construct. Alternatively, we can also use a builder,</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">inferpy</span> <span class="k">import</span> <span class="n">ProbModel</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">ProbModel</span><span class="p">(</span><span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">mu0</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">w_n</span><span class="p">,</span><span class="n">x_n</span><span class="p">])</span>
<span class="n">pca</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
<p>The model must be <strong>compiled</strong> before it can be used.</p>
<p>Like any random variable object, a probabilistic model is equipped with
methods such as <code class="docutils literal"><span class="pre">log\_prob()</span></code> and <code class="docutils literal"><span class="pre">sample()</span></code>. Then, we can sample data
from the model and compute the log-likelihood of a data set:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">log_like</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>Folowing Edward&#8217;s approach, a random variable <span class="math">\(x\)</span> is associated to
a tensor <span class="math">\(x^*\)</span> in the computational graph handled by TensorFlow,
where the computations takes place. This tensor <span class="math">\(x^*\)</span> contains the
samples of the random variable <span class="math">\(x\)</span>, i.e.
<span class="math">\(x^*\sim p(x|\theta)\)</span>. In this way, random variables can be
involved in expressive deterministic operations. For example, the
following piece of code corresponds to a zero inflated linear regression
model</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#Prior</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
<span class="n">w0</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#Likelihood model</span>
<span class="k">with</span> <span class="n">inf</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">Binomial</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">w0</span> <span class="o">+</span> <span class="n">inf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span> <span class="n">transpose_b</span> <span class="o">=</span> <span class="n">true</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">Delta</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">Deterministic</span><span class="p">(</span><span class="n">h</span><span class="o">*</span><span class="n">y0</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">h</span><span class="p">)</span><span class="o">*</span><span class="n">y1</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>

<span class="n">probmodel</span> <span class="o">=</span> <span class="n">ProbModel</span><span class="p">(</span><span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span><span class="n">w0</span><span class="p">,</span><span class="n">p</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">y0</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
<span class="n">probmodel</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">probmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>A special case, it is the inclusion of deep neural networks within our
probabilistic model to capture complex non-linear dependencies between
the random variables. This is extensively treated in the the Guide to
Bayesian Deep Learning.</p>
<p>Finally, a probablistic model have the following methods:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">probmodel.summary()</span></code>: prints a summary representation of the
model.</li>
<li><code class="docutils literal"><span class="pre">probmodel.get_config()</span></code>: returns a dictionary containing the
configuration of the model. The model can be reinstantiated from its
config via:</li>
</ul>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
<span class="n">probmodel</span> <span class="o">=</span> <span class="n">ProbModel</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">model.to_json()</span></code>: returns a representation of the model as a JSON
string. Note that the representation does not include the weights,
only the architecture. You can reinstantiate the same model (with
reinitialized weights) from the JSON string via: ```python from
models import model_from_json</li>
</ul>
<p>json_string = model.to_json() model = model_from_json(json_string)
```</p>
</div>
<hr class="docutils" />
<div class="section" id="guide-to-approximate-inference-in-probabilistic-models">
<h2>Guide to Approximate Inference in Probabilistic Models<a class="headerlink" href="#guide-to-approximate-inference-in-probabilistic-models" title="Permalink to this headline">¶</a></h2>
<p>The API defines the set of algorithms and methods used to perform
inference in a probabilistic model <span class="math">\(p(x,z,\theta)\)</span> (where
<span class="math">\(x\)</span> are the observations, <span class="math">\(z\)</span> the local hidden variibles,
and <span class="math">\(\theta\)</span> the global parameters of the model). More precisely,
the inference problem reduces to compute the posterior probability over
the latent variables given a data sample
<span class="math">\(p(z,\theta | x_{train})\)</span>, because by looking at these
posteriors we can uncover the hidden structure in the data. For the
running example, <span class="math">\(p(\mu|x_{train})\)</span> tells us where the centroids of
the data are, while <span class="math">\(p(z_n|x_{train})\)</span> shows us to which centroid
every data point belongs to.</p>
<p>InferPy inherits Edward&#8217;s approach an consider approximate inference
solutions,</p>
<div class="math">
\[q(z,\theta) \approx p(z,\theta | x_{train})\]</div>
<p>in which the task is to approximate the posterior
<span class="math">\(p(z,\theta | x_{train})\)</span> using a family of distributions,
<span class="math">\(q(z,\theta; \lambda)\)</span>, indexed by a parameter vector
<span class="math">\(\lambda\)</span>.</p>
<p>A probabilistic model in InferPy should be compiled before we can access
these posteriors,</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">probmodel</span> <span class="o">=</span> <span class="n">ProbModel</span><span class="p">(</span><span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">z_n</span><span class="p">,</span> <span class="n">x_n</span><span class="p">])</span>
<span class="n">probmodel</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">infMethod</span> <span class="o">=</span> <span class="s1">&#39;KLqp&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">posterior_mu</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
</pre></div>
</div>
<p>The compilation process allows to choose the inference algorithm through
the <code class="docutils literal"><span class="pre">infMethod</span></code> argument. In the above example we use <code class="docutils literal"><span class="pre">'Klqp'</span></code>. Other
inference algorithms include: <code class="docutils literal"><span class="pre">'NUTS'</span></code>, <code class="docutils literal"><span class="pre">'MCMC'</span></code>, <code class="docutils literal"><span class="pre">'KLpq'</span></code>, etc. Look at ? for
a detailed description of the available inference algorithms.</p>
<p>Following InferPy guiding principles, users can further configure the
inference algorithm.</p>
<p>First, they can define they family &#8216;Q&#8217; of approximating distributions,</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">probmodel</span> <span class="o">=</span> <span class="n">ProbModel</span><span class="p">(</span><span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">z_n</span><span class="p">,</span><span class="n">x_n</span><span class="p">])</span>

<span class="n">q_z_n</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">Multinomial</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">z_n</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_unifrom&#39;</span><span class="p">)</span>
<span class="n">q_mu</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">PointMass</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">mu</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_unifrom&#39;</span><span class="p">)</span>
<span class="n">q_sigma</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">PointMass</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>

<span class="n">probmodel</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">infMethod</span> <span class="o">=</span> <span class="s1">&#39;KLqp&#39;</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">q_sigma</span><span class="p">,</span> <span class="n">q_z_n</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">posterior_mu</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, the posterior <strong>q</strong> belongs to the same distribution family
than <strong>p</strong> , but in the above example we show how we can change that
(e.g. we set the posterior over <strong>mu</strong> to obtain a point mass estimate
instead of the Gaussian approximation used by default). We can also
configure how these <strong>q&#8217;s</strong> are initialized using any of the Keras&#8217;s
initializers.</p>
<p>Inspired by Keras semantics, we can furhter configure the inference
algorithm,</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">probmodel</span> <span class="o">=</span> <span class="n">ProbModel</span><span class="p">(</span><span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">z_n</span><span class="p">,</span><span class="n">x_n</span><span class="p">])</span>

<span class="n">q_z_n</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">Multinomial</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">z_n</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_unifrom&#39;</span><span class="p">)</span>
<span class="n">q_mu</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">PointMass</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">mu</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_unifrom&#39;</span><span class="p">)</span>
<span class="n">q_sigma</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">PointMass</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>

<span class="n">sgd</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">infkl_qp</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">KLqp</span><span class="p">(</span><span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">q_sigma</span><span class="p">,</span> <span class="n">q_z_n</span><span class="p">],</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;ELBO&quot;</span><span class="p">)</span>
<span class="n">probmodel</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">infMethod</span> <span class="o">=</span> <span class="n">infkl_qp</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">posterior_mu</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
</pre></div>
</div>
<p>Have a look at Inference Zoo to explore other configuration options.</p>
<p>In the last part of this guide, we highlight that InferPy directly
builds on top of Edward&#8217;s compositionality idea to design complex
infererence algorithms.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">probmodel</span> <span class="o">=</span> <span class="n">ProbModel</span><span class="p">(</span><span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">z_n</span><span class="p">,</span><span class="n">x_n</span><span class="p">])</span>

<span class="n">q_z_n</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">Multinomial</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">z_n</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_unifrom&#39;</span><span class="p">)</span>
<span class="n">q_mu</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">PointMass</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">mu</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_unifrom&#39;</span><span class="p">)</span>
<span class="n">q_sigma</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">PointMass</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>

<span class="n">infkl_qp</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">KLqp</span><span class="p">(</span><span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="n">q_z_n</span><span class="p">],</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="n">innerIter</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">infMAP</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">MAP</span><span class="p">(</span><span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">q_sigma</span><span class="p">],</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="s1">&#39;sgd&#39;</span><span class="p">)</span>

<span class="n">probmodel</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">infMethod</span> <span class="o">=</span> <span class="p">[</span><span class="n">infkl_qp</span><span class="p">,</span><span class="n">infMAP</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">posterior_mu</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
</pre></div>
</div>
<p>With the above sintaxis, we perform a variational EM algorithm, where
the E step is repeated 10 times for every MAP step.</p>
<p>More flexibility is also available by defining how each mini-batch is
processed by the inference algorithm. The following piece of code is
equivalent to the above one,</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">probmodel</span> <span class="o">=</span> <span class="n">ProbModel</span><span class="p">(</span><span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">z_n</span><span class="p">,</span><span class="n">x_n</span><span class="p">])</span>

<span class="n">q_z_n</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">Multinomial</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">z_n</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_unifrom&#39;</span><span class="p">)</span>
<span class="n">q_mu</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">PointMass</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">mu</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_unifrom&#39;</span><span class="p">)</span>
<span class="n">q_sigma</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">PointMass</span><span class="p">(</span><span class="n">bind</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>

<span class="n">infkl_qp</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">KLqp</span><span class="p">(</span><span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="n">q_z_n</span><span class="p">])</span>
<span class="n">infMAP</span> <span class="o">=</span> <span class="n">inf</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">MAP</span><span class="p">(</span><span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="n">q_mu</span><span class="p">,</span> <span class="n">q_sigma</span><span class="p">])</span>

<span class="n">emAlg</span> <span class="o">=</span> <span class="k">lambda</span> <span class="p">(</span><span class="n">infMethod</span><span class="p">,</span> <span class="n">dataBatch</span><span class="p">):</span>
   <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
       <span class="n">infMethod</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">dataBatch</span><span class="p">)</span>

   <span class="n">infMethod</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">dataBatch</span><span class="p">)</span>
   <span class="k">return</span>

<span class="n">probmodel</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">infMethod</span> <span class="o">=</span> <span class="p">[</span><span class="n">infkl_qp</span><span class="p">,</span><span class="n">infMAP</span><span class="p">],</span> <span class="n">ingAlg</span> <span class="o">=</span> <span class="n">emAlg</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">posterior_mu</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
</pre></div>
</div>
<p>Have a look again at Inference Zoo to explore other complex
compositional options.</p>
</div>
<hr class="docutils" />
<div class="section" id="guide-to-bayesian-deep-learning">
<h2>Guide to Bayesian Deep Learning<a class="headerlink" href="#guide-to-bayesian-deep-learning" title="Permalink to this headline">¶</a></h2>
<p>InferPy inherits Edward&#8217;s approach for representing probabilistic models
as (stochastic) computational graphs. As describe above, a random
variable <span class="math">\(x\)</span> is associated to a tensor <span class="math">\(x^*\)</span> in the
computational graph handled by TensorFlow, where the computations takes
place. This tensor <span class="math">\(x^*\)</span> contains the samples of the random
variable <span class="math">\(x\)</span>, i.e. <span class="math">\(x^* \sim p(x|\theta)\)</span>. In this way,
random variables can be involved in complex deterministic operations
containing deep neural networks, math operations and another libraries
compatible with Tensorflow (such as Keras).</p>
<p>Bayesian deep learning or deep probabilistic programming enbraces the
idea of employing deep neural networks within a probabilistic model in
order to capture complex non-linear dependencies between variables.</p>
<p>InferPy&#8217;s API gives support to this powerful and flexible modelling
framework. Let us start by showing how a variational autoencoder over
binary data can be defined by mixing Keras and InferPy code.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span>

<span class="n">M</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">dim_z</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">dim_x</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1">#Define the decoder network</span>
<span class="n">input_z</span>  <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">dim_z</span><span class="p">)</span>
<span class="n">layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">input_z</span><span class="p">)</span>
<span class="n">output_x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dim_x</span><span class="p">)(</span><span class="n">layer</span><span class="p">)</span>
<span class="n">decoder_nn</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">output_x</span><span class="p">)</span>

<span class="c1">#define the generative model</span>
<span class="k">with</span> <span class="n">inf</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="n">N</span><span class="p">)</span>
 <span class="n">z</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">dim_z</span><span class="p">)</span>
 <span class="n">x</span> <span class="o">=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">logits</span> <span class="o">=</span> <span class="n">decoder_nn</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">value</span><span class="p">()),</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>

<span class="c1">#define the encoder network</span>
<span class="n">input_x</span>  <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">d_x</span><span class="p">)</span>
<span class="n">layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">input_x</span><span class="p">)</span>
<span class="n">output_loc</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dim_z</span><span class="p">)(</span><span class="n">layer</span><span class="p">)</span>
<span class="n">output_scale</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dim_z</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;softplus&#39;</span><span class="p">)(</span><span class="n">layer</span><span class="p">)</span>
<span class="n">encoder_loc</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">output_mu</span><span class="p">)</span>
<span class="n">encoder_scale</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">output_scale</span><span class="p">)</span>

<span class="c1">#define the Q distribution</span>
<span class="n">q_z</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">encoder_loc</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">value</span><span class="p">()),</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">encoder_scale</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">value</span><span class="p">()))</span>

<span class="c1">#compile and fit the model with training data</span>
<span class="n">probmodel</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">infMethod</span> <span class="o">=</span> <span class="s1">&#39;KLqp&#39;</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="p">{</span><span class="n">z</span> <span class="p">:</span> <span class="n">q_z</span><span class="p">})</span>
<span class="n">probmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="c1">#extract the hidden representation from a set of observations</span>
<span class="n">hidden_encoding</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">targetvar</span> <span class="o">=</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, the parameters of the encoder and decoder neural networks
are automatically managed by Keras. These parameters are them treated as
model parameters and not exposed to the user. In consequence, we can not
be Bayesian about them by defining specific prior distributions. In this
example (?) , we show how we can avoid that by introducing extra
complexity in the code.</p>
<p>Other examples of probabilisitc models using deep neural networks are: -
Bayesian Neural Networks - Mixture Density Networks - ...</p>
<p>We can also define a Keras model whose input is an observation and its
output its the expected value of the posterior over the hidden
variables, <span class="math">\(E[p(z|x)]\)</span>, by using the method <code class="docutils literal"><span class="pre">toKeras</span></code>, as a way to
create more expressive models.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Sequential</span>

<span class="c1">#We define a Keras&#39; model whose input is data sample &#39;x&#39; and the output is the encoded vector E[p(z|x)]</span>
<span class="n">variational_econder_keras</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">toKeras</span><span class="p">(</span><span class="n">targetvar</span> <span class="o">=</span> <span class="n">z</span><span class="p">)</span>

<span class="n">vision_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">vision_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">))</span>
<span class="n">vision_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">vision_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">vision_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>

<span class="c1"># Now let&#39;s get a tensor with the output of our vision model:</span>
<span class="n">encoded_image</span> <span class="o">=</span> <span class="n">vision_model</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>

<span class="c1"># Let&#39;s concatenate the vae vector and the convolutional image vector:</span>
<span class="n">merged</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">variational_econder_keras</span><span class="p">,</span> <span class="n">encoded_image</span><span class="p">])</span>

<span class="c1"># And let&#39;s train a logistic regression over 100 categories on top:</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">merged</span><span class="p">)</span>

<span class="c1"># This is our final model:</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_x</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># The next stage would be training this model on actual data.</span>
</pre></div>
</div>
</div>
<div class="section" id="guide-to-validation-of-probabilistic-models">
<h2>Guide to Validation of Probabilistic Models<a class="headerlink" href="#guide-to-validation-of-probabilistic-models" title="Permalink to this headline">¶</a></h2>
<p>Model validation try to assess how faifhfully the inferered
probabilistic model represents and explain the observed data.</p>
<p>The main tool for model validation consists on analyzing the posterior
predictive distribution,</p>
<p><span class="math">\(p(y_{test}, x_{test}|y_{train}, x_{train}) = \int p(y_{test}, x_{test}|z,\theta)p(z,\theta|y_{train}, x_{train}) dzd\theta\)</span></p>
<p>This posterior predictive distribution can be used to measure how well
the model fits an independent dataset using the test marginal
log-likelihood, <span class="math">\(\ln p(y_{test}, x_{test}|y_{train}, x_{train})\)</span>,</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">log_like</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;log_likelihood&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>In other cases, we may need to evalute the predictive capacity of the
model with respect to some target variable <span class="math">\(y\)</span>,</p>
<p><span class="math">\(p(y_{test}|x_{test}, y_{train}, x_{train}) = \int p(y_{test}|x_{test},z,\theta)p(z,\theta|y_{train}, x_{train}) dzd\theta\)</span></p>
<p>So the metrics can be computed with respect to this target variable by
using the ‘targetvar’ argument,</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">log_like</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">mse</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">targetvar</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;log_likelihood&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;mse&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>So, the log-likelihood metric as well as the accuracy and the mean
square error metric are computed by using the predictive posterior
<span class="math">\(p(y_{test}|x_{test}, y_{train}, x_{train})\)</span>.</p>
<p>Custom evaluation metrics can also be defined,</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="n">observations</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">map_fn</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">getMean</span><span class="p">(),</span> <span class="n">posterior</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

<span class="n">mse</span><span class="p">,</span> <span class="n">mae</span> <span class="o">=</span> <span class="n">probmodel</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">targetvar</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">])</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="guide-to-data-handling">
<h2>Guide to Data Handling<a class="headerlink" href="#guide-to-data-handling" title="Permalink to this headline">¶</a></h2>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">inferpy</span> <span class="k">as</span> <span class="nn">inf</span>
<span class="kn">from</span> <span class="nn">inferpy.models</span> <span class="k">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">InverseGamma</span><span class="p">,</span> <span class="n">Dirichlet</span>

<span class="c1">#We first define the probabilistic model</span>
<span class="k">with</span> <span class="n">inf</span><span class="o">.</span><span class="n">ProbModel</span><span class="p">()</span> <span class="k">as</span> <span class="n">mixture_model</span><span class="p">:</span>
    <span class="c1"># K defines the number of components.</span>
    <span class="n">K</span><span class="o">=</span><span class="mi">10</span>
    <span class="c1">#Prior for the means of the Gaussians</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
    <span class="c1">#Prior for the precision of the Gaussians</span>
    <span class="n">invgamma</span> <span class="o">=</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="n">concentration</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rate</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">K</span><span class="p">,</span><span class="n">d</span><span class="p">])</span>
    <span class="c1">#Prior for the mixing proportions</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>

    <span class="c1"># Number of observations</span>
    <span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="c1">#data Model</span>
    <span class="k">with</span> <span class="n">inf</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="n">N</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
        <span class="c1"># Sample the component indicator of the mixture. This is a latent variable that can not be observed</span>
        <span class="n">z_n</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span> <span class="o">=</span> <span class="n">theta</span><span class="p">)</span>
        <span class="c1"># Sample the observed value from the Gaussian of the selected component.</span>
        <span class="n">x_n</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">z_n</span><span class="p">),</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">invgamma</span><span class="p">,</span><span class="n">z_n</span><span class="p">),</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>

<span class="c1">#compile the probabilistic model</span>
<span class="n">mixture_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">infAlg</span> <span class="o">=</span> <span class="s1">&#39;klqp&#39;</span><span class="p">)</span>

<span class="c1">#fit the model with data</span>
<span class="n">mixture_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="probzoo.html" class="btn btn-neutral float-right" title="Probabilistic Model Zoo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gettingstarted.html" class="btn btn-neutral" title="Getting Started" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Andrés R. Masegosa, Rafael Cabañas.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>